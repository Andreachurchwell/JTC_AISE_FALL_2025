This repo is a big simulation of:

ğŸš¶ Humans vs ğŸ¤– Bots
ğŸ‘‰ Performing actions on a fake website
ğŸ§  A system detecting who is human vs who is a bot
ğŸ“Œ In real time
âš¡ Using streaming ML

So instead of data stored in files,
data is flowing all the time like a river.

ğŸ§± What Are The Pieces?

ğŸ§© 1. Traffic Simulator â†’ â€œCreates Fake Peopleâ€
It sends events like:
mouse moved
clicked something
scrolled
typed

Two types:

âœ” humans (random, messy behavior)
âœ” bots (very predictable patterns)

ğŸ¯ This is fake data but realistic enough.
-----------------------------------------------------------------------------

ğŸ§© 2. Kafka â†’ â€œMailman Delivering Eventsâ€
Kafka takes all incoming data and holds it temporarily.
Think:
Busy mailbox storing events until someone processes them.
You can go to Kafka UI:
ğŸ‘‰ http://localhost:18080

----------------------------------------------------------------------------

ğŸ§© 3. Flink Processor â†’ â€œThe Real-Time Feature Calculationâ€
This part:
watches events as they stream in
looks per session
builds behavioral patterns

Some examples:

Behavior                              	Meaning
mouse path smoothness	             humans wiggle
click velocity	                     bots click evenly
timing irregularity	                 humans delay inconsistently

This is the heart of the ML pipeline.
Look into folder: flink-processor/

----------------------------------------------------------------------------

ğŸ§© 4. Spark Processor
This is an optional copy of Flink logic.
Think of it as:
â­ Same features
â­ Different tool
âœ” Good for comparing which works better

----------------------------------------------------------------------------

ğŸ§© 5. Feature Store
This is just storage:

Storage	                                             Purpose
Redis	                                             FAST! Online lookup so API is quick
PostgreSQL	                                         Historical data for training

You can think:
Redis = "cache for predictions now"
PostgreSQL = "everything recorded forever"

-----------------------------------------------------------------------------

ğŸ§© 6. Bot Detection API â†’ â€œMakes predictionsâ€

Based on features retrieved, it returns:
âœ” bot probability
âœ” whether to show CAPTCHA
âœ” session scores

Example response:{
  "bot_probability": 0.85,
  "should_challenge": true
}

-----------------------------------------------------------------------------

ğŸ§© 7. CAPTCHA Service

If probability is high â†’ show challenge
If challenge solved â†’ mark session safe

âœ¨ Adaptive, not shown to everyone âœ¨

-----------------------------------------------------------------------------

ğŸ§  Spark in THIS Repo = â€œAlternative Feature Engineâ€

There are two processors:

ğŸŸ£ Flink â†’ main pipeline
ğŸŸ¡ Spark â†’ alternative pipeline

Think of Spark as:

Another way to compute the same user behavior signals,
but using micro-batches instead of event-by-event updates.

ğŸ§  How Spark Works (Simple Version)
Instead of processing events one-by-one like Flink does,
Spark groups data into batches such as:

ğŸ“¦ â€œevery 5 secondsâ€
ğŸ“¦ â€œevery 10 secondsâ€

Then it calculates the same features you saw in Flink:
click velocity
number of actions in a session
timing patterns
mouse movement stats
and writes them to:

ğŸŸ¢ Redis (online, fast reads)
ğŸŸ¢ PostgreSQL (historical records)

"Spark is basically doing things in small batches instead of reacting to every single event one at a time like Flink does. So it's kind of a different way of getting the same information. Having both lets us see which approach works betterâ€”like which is faster, more accurate, and handles mistakes or slowdowns better."

--------------------------------------------------------------------

Why is Spark in here?

Say:

â€œSpark gives us a batch-based version of the same feature processing so we can compare it to Flink and see which one works better.â€

When someone asks:
What does Spark actually do?

Say:

â€œIt takes user events, groups them by session and time window, calculates behavior features, and writes them out so the model can score each session.â€

â€œIn our repo, Spark is basically another path for computing behavior-based features from live user events. It starts a SparkSession, subscribes to the Kafka topic, reads events continuously, and groups them into very small time-based batches. Inside those batches it computes the same behavioral features that the Flink pipeline calculatesâ€”things like event counts, click velocity, timing gaps, etc. Then Spark writes those features out to Redis for fast lookups and to PostgreSQL to store history. The idea is that Flink is real-time event-by-event, while Spark batches events every few seconds. Having both engines in the repo lets us compare how streaming pipelines behave under two different strategies.â€

â€œSpark does the same feature engineering as Flink, but instead of reacting instantly to every single event, Spark processes events in tiny time windows (micro-batches). This is useful because we can compare latency, throughput, and accuracy between batch-style streaming and real-time streaming.â€


--------------------------------------------------------------------------------

Apache Spark is an open-source engine built in 2009 for analyzing huge amounts of data.
Before Spark, companies mainly used MapReduce, but it was slow because every step had to be written back to disk. Spark solved that problem by running in memory, which made it up to about 100x faster. That was a huge game-changer.

Today, companies like NASA, Amazon, and over 80% of Fortune 500 organizations use Spark for large-scale data processing.

Spark can run locally on your laptop or across multiple machines using Kubernetes to scale horizontally.

When you use Spark, you typically:
start a Spark session
load data into memory
process that data using either DataFrames or SQL
Spark also includes its own machine-learning library called MLlib, which lets you train models directly inside Spark without switching to another tool.

â€œSpark basically came in because companies went from storing tiny amounts of data to petabytes. MapReduce existed, but it was slow because everything hit the disk. Spark changed thatâ€”it does stuff in memory, which made it way faster. Thatâ€™s why Amazon, NASA, and tons of Fortune 500 companies use it.

You can even run Spark locally, right on your machine. You start a Spark session, load data into memory, and it becomes a DataFrame that you can query like SQL or use for ML.

Spark also comes with MLlib, which is built-in machine learningâ€¦ so Spark can handle data and train models at massive scale. And if you need more power, Kubernetes can scale Spark horizontally.â€