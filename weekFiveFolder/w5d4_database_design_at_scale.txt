Overview
This class covered why databases need horizontal scaling under high traffic, the differences between replication and sharding, CAP theorem tradeoffs, designing partitionable schemas, and choosing between read replicas and write masters.

Key Definitions
Replication

Keeping multiple copies of data (one primary, many replicas) for HA and read scaling.

Sharding

Splitting a dataset into horizontal partitions (shards) across servers/nodes.

CAP Theorem

States a distributed system can only guarantee 2 of 3: Consistency, Availability, Partition Tolerance.

Hotspot Key

A shard key value that receives disproportionate traffic, causing imbalance.

Replication Lag

Time delay for replica to apply changes from primary.

Failover

Automatic switch to replica if primary fails.

Consistent Hashing

Algorithm for distributing keys evenly across shards, minimizing reshuffling.




ğŸ§± The Real Basics â€” Whatâ€™s Going On Here
ğŸ’¡ Imagine: You own a small website

Letâ€™s say you built a tiny website that shows the weather in your town.

At first:

All your data (cities, temps, forecasts) lives in one database on one computer.
Thatâ€™s fine because only a few people use it.

Now imagine your app gets popular â€” thousands of people are checking it every second.
That one computer starts struggling. Itâ€™s like one cashier trying to handle a huge line of customers.

So now you have to scale your database â€” help it handle more people without crashing.

ğŸ§© The Two Main Ways to Scale
1ï¸âƒ£ Replication = Make Copies

Think of this like:

â€œIf one cashier is overwhelmed, hire a few more cashiers with the same register.â€

In tech:

You copy your data to other computers (replicas).

People can read from any copy.

Thereâ€™s still one main database that accepts new changes â€” called the primary.

ğŸ§  Why?

If one goes down, the others still work (thatâ€™s availability).

You can handle more people reading data at once.

âš ï¸ Butâ€¦ sometimes the copies take a second to catch up â†’ replication lag.

2ï¸âƒ£ Sharding = Split the Data

Now imagine:

â€œInstead of every cashier handling all customers, each one gets part of the line â€” Aâ€“M, Nâ€“Z.â€

In tech:

You divide your big table into smaller parts (called shards).
Example:

Shard 1 = people in Tennessee

Shard 2 = people in New York

Shard 3 = people in California

Each shard lives on a different computer.

ğŸ§  Why?

Itâ€™s faster â€” each server handles less data.

You can add more shards as you grow.

âš ï¸ Butâ€¦ now data is spread out, so finding and updating it is more complicated.

âš–ï¸ The CAP Theorem â€” The â€œTrade-Off Ruleâ€

When you have many computers working together, sometimes a network breaks or one machine goes down.
In that moment, you can only guarantee two out of three:

Consistency â€“ Everyone sees the same, up-to-date data.

Availability â€“ The system keeps working even if one part fails.

Partition tolerance â€“ The system can survive if the computers canâ€™t talk to each other.

ğŸ§  Example:
If the TN shard loses connection, do you:

Wait until it reconnects (so all data matches â†’ consistency)

Or let users still see last-known data (keep it up â†’ availability)

You have to pick. Thatâ€™s the CAP trade-off.

ğŸŒ Real World Examples
Concept	Real Example
Replication	Instagram makes copies of your photos across servers so you can still see them if one goes down.
Sharding	Netflix splits users by region so not all movies and accounts live in one database.
CAP Theorem	Amazon might show slightly outdated product stock for a second instead of crashing.





ğŸ§± Part 2 â€” Horizontal Scaling and How Data Gets Distributed
âš™ï¸ 1ï¸âƒ£ Vertical vs Horizontal Scaling

Vertical Scaling â†’ â€œMake one machine stronger.â€

You upgrade the same server â€” add more memory, faster CPU, bigger disk.

Works for a while, but thereâ€™s a limit (you canâ€™t make one computer infinite).

Horizontal Scaling â†’ â€œAdd more machines.â€

Instead of one super-computer, you use lots of regular ones working together.

Each stores part of the data or handles part of the traffic.

This is what sharding and replication make possible.

ğŸ§  Example
Your weather app starts small â†’ one server.
Then you add more servers:

One stores Tennessee weather, another stores Florida, another stores California.
Thatâ€™s horizontal scaling â€” spreading the load sideways, not piling it up on one box.

ğŸ¯ 2ï¸âƒ£ The Goal of Scaling

You want three things:

The site stays fast.

It stays up (even if one machine dies).

You can add more capacity easily later.

Replication gives you availability.
Sharding gives you capacity.

ğŸ”¢ 3ï¸âƒ£ How to Decide What Goes Where

When you shard, you have to decide which record goes to which server.
Thatâ€™s where a shard key comes in â€” some piece of info you use to split the data.

ğŸ§  Example
If you choose â€œcity nameâ€ as the key:

Nashville â†’ Server 1

New York â†’ Server 2

Los Angeles â†’ Server 3

If one key (say â€œNew Yorkâ€) gets way more traffic than others, that shard gets overloaded.
Thatâ€™s called a hotspot key â€” one server doing all the work.

ğŸ”„ 4ï¸âƒ£ Consistent Hashing (plain version)

Now, what happens when you add or remove servers?
You donâ€™t want to reshuffle all your data every time.

Consistent hashing is a smart way to spread data out so:

New servers only take over a small portion of existing data.

You donâ€™t have to move everything.

Think of a circle:

Each server owns a slice of the circle.

Each data item (user, city, etc.) lands in the nearest slice.
Add a new server â†’ you only shift the data near that slice.

It keeps things balanced and stable when the system grows.

ğŸš¨ 5ï¸âƒ£ Failover and Replication Lag in Simple Terms

Failover â†’ if one primary database crashes, a replica instantly becomes the new primary.
Replication Lag â†’ a tiny delay before replicas get the latest updates.
So right after a change, one server might still have the â€œoldâ€ version for a moment.




| Concept                | What it means             | Why it matters          |
| ---------------------- | ------------------------- | ----------------------- |
| **Vertical Scaling**   | Make one machine stronger | Simple, but limited     |
| **Horizontal Scaling** | Add more machines         | Flexible, scalable      |
| **Replication**        | Copies of data            | Backup + faster reads   |
| **Sharding**           | Split data                | Handles more traffic    |
| **Shard Key**          | Rule for dividing data    | Needs to be balanced    |
| **Hotspot Key**        | One key gets all traffic  | Causes slowdowns        |
| **Consistent Hashing** | Smart balancing method    | Avoids big reshuffles   |
| **Failover**           | Backup takes over         | Keeps system online     |
| **Replication Lag**    | Delay syncing copies      | Causes brief mismatches |


ğŸŒ¦ï¸ The Weather App Story â€” Scaling Step by Step
1ï¸âƒ£ The Beginning

You build a simple weather app.

One small database lives on one computer.

It has one table with all the cities and their forecasts.

Only a few people use it â€” easy peasy.

2ï¸âƒ£ Traffic starts to grow

Now hundreds of people are checking the weather at the same time.
That one database is getting slower.
Itâ€™s like one person answering calls for the whole country.

So what can you do?
â†’ You make copies of your data â€” thatâ€™s replication.

3ï¸âƒ£ You add replication

You decide:

One database will be the primary â€” it handles all the updates (when new data comes in).

Two more will be replicas â€” exact copies that handle people reading the data.

So when users check the weather:

Some requests go to the main database.

Others go to the replicas to spread the load.

ğŸ§  This keeps your app fast and available even if one server crashes.
âš ï¸ But sometimes, it takes a second for replicas to catch up â†’ replication lag.

4ï¸âƒ£ Your app keeps growing

Now thousands (or millions!) of users are using it.
Even with replication, one database canâ€™t hold all the worldâ€™s data anymore.

Thatâ€™s when you use sharding.

5ï¸âƒ£ You add sharding

You decide to split your data based on location:

Shard 1: Eastern U.S. cities

Shard 2: Central U.S. cities

Shard 3: Western U.S. cities

Each shard has its own primary and replicas.

Now your database system looks like this:

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Load    â”‚
                â”‚ Balancer â”‚
                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
  â”‚  Shard 1    â”‚       â”‚  Shard 2    â”‚
  â”‚ East Cities  â”‚       â”‚ Central Cities â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Primary + Repâ”‚       â”‚ Primary + Rep â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


So when someone checks Nashville, the app knows:

â€œGo ask Shard 2, thatâ€™s where Tennessee data lives.â€

ğŸ§  Now itâ€™s faster and more organized.

6ï¸âƒ£ Problem: One shard gets too busy

Letâ€™s say everyone suddenly starts checking New York because of a snowstorm.
That means Shard 1 (East) is overloaded.
Thatâ€™s called a hotspot key â€” one shard doing too much work.

You fix it by using consistent hashing â€” a smarter way to balance data evenly so no shard gets left with all the heavy traffic.

7ï¸âƒ£ Something breaks â€” but the app stays up

One of your primary databases crashes.
No worries â€” one of its replicas automatically becomes the new primary.
Thatâ€™s failover â€” automatic backup handling.

Users never even notice.
Thatâ€™s why systems like Amazon, Netflix, and OpenWeather do this.

8ï¸âƒ£ The CAP trade-off in your app

Now imagine thereâ€™s a storm and the internet link between shards goes down.
Your app has to choose:

Should it keep showing old data (availability)?

Or wait until all servers sync again (consistency)?

You canâ€™t have both 100% â€” thatâ€™s the CAP theorem in action.
Your app might show slightly old weather but stay online â†’ thatâ€™s choosing availability + partition tolerance.

âœ… End Result

Now your weather app is:

Replicated (safe and fast)

Sharded (split and scalable)

Balanced (no overloaded servers)

Available (still runs even if one piece fails)

Thatâ€™s how big tech companies design systems to handle millions of users.

